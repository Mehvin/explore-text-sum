{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing datasets created!\n"
     ]
    }
   ],
   "source": [
    "#This program gets the testing dataset (30%) from abstractive methods \n",
    "#to be used for extractive text summarization\n",
    "\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "cnn_stories_dir = \"/home/kawaijoe/Dataset/cnn-dailymail-master/cnn/stories\"\n",
    "dm_stories_dir = \"/home/kawaijoe/Dataset/cnn-dailymail-master/dailymail/stories\"\n",
    "url_file =  \"/home/kawaijoe/Dataset/cnn-dailymail-master/url_lists/final_test.txt\"\n",
    "testing_set_for_extractive = \"/home/kawaijoe/Dataset/cnn-dailymail-master/pre_extractive_dataset\"\n",
    "\n",
    "os.makedirs(testing_set_for_extractive)\n",
    "\n",
    "h = hashlib.sha1()\n",
    "\n",
    "def read_text_file(text_file):\n",
    "    lines = []\n",
    "    with open(text_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            lines.append(line.strip())\n",
    "    return lines\n",
    "\n",
    "def hashhex(s):\n",
    "    #Returns a heximal formated SHA1 hash of the input string.\n",
    "    h = hashlib.sha1()\n",
    "    h.update(s)\n",
    "    return h.hexdigest()\n",
    "\n",
    "\n",
    "def get_url_hashes(url_list):\n",
    "  return [hashhex(url) for url in url_list]\n",
    "\n",
    "def get_file_data(url_hash):\n",
    "    retrieved = False\n",
    "    data = \"\"\n",
    "    \n",
    "    for file in os.listdir(cnn_stories_dir):\n",
    "        url_hash_file = url_hash + \".story\"\n",
    "        if url_hash_file in file:\n",
    "            data = \"\"\n",
    "            with open(cnn_stories_dir + '/' + file, 'r') as f:\n",
    "                for line in f:\n",
    "                    data += line\n",
    "            retrieved = True\n",
    "            return data\n",
    "\n",
    "    if retrieved == False:\n",
    "            for file in os.listdir(dm_stories_dir):\n",
    "                url_hash_file = url_hash + \".story\"\n",
    "                if url_hash_file in file:\n",
    "                    data = \"\"\n",
    "                    with open(dm_stories_dir + '/' + file, 'r') as f:\n",
    "                        for line in f:\n",
    "                            data += line\n",
    "                    retrieved = True\n",
    "                    return data\n",
    "\n",
    "url_list = read_text_file(url_file)\n",
    "url_hashes = get_url_hashes(url_list)\n",
    "\n",
    "i = 0\n",
    "\n",
    "while i < len(url_hashes):\n",
    "    url_hash = url_hashes[i]\n",
    "    \n",
    "    new_filename = url_hash\n",
    "    new_filepath = os.path.join(testing_set_for_extractive, new_filename)\n",
    "    \n",
    "    data = get_file_data(url_hash)\n",
    "    \n",
    "    with open(new_filepath, \"a\") as f:\n",
    "        f.write(str(data))\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "print \"Testing datasets created!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
