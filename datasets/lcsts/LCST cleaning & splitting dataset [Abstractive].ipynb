{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCSTS: A Large-Scale Chinese Short Text Summarization Dataset\n",
    "\n",
    "Dataset constructed from the Chinese microblogging website SinaWeibo.\n",
    "\n",
    "The corpus consists of over 2 million real Chinese short texts with short summaries given by the writer of each text.\n",
    "\n",
    "\n",
    "Link - http://icrc.hitsz.edu.cn/Article/show/139.html\n",
    "\n",
    "Download link - https://www.dropbox.com/s/pneadrav0lhz2l0/LCSTS2.0.zip?dl=0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PART_I.txt data is formatted in this way\n",
    "\n",
    "<doc id=0>\n",
    "    <summary>\n",
    "        林志颖公司疑涉虚假营销...\n",
    "    </summary>\n",
    "    <short_text>\n",
    "        日前，方舟子发文直指林志颖旗下爱碧丽推销假保健品，引起哗然....\n",
    "    </short_text>\n",
    "</doc>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import jieba #Chinese text segmentation tool - used to properly segment Chinese text\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'PART_I.txt'\n",
    "\n",
    "extractive_dir = \"/home/s10166858/kawaijoe/Dataset/LCST/extractive_dataset\"\n",
    "reference_dir = \"/home/s10166858/kawaijoe/Dataset/LCST/reference\"\n",
    "training_dir = \"/home/s10166858/kawaijoe/Dataset/LCST/abstractive_dataset/training_set\"\n",
    "testing_dir = \"/home/s10166858/kawaijoe/Dataset/LCST/abstractive_dataset/testing_set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Works for PART_I.txt (For PART_II.txt & PART_III.txt need to change math algorithm)\n",
    "def create_abstractive_files(fileName, reference_dir, extractive_dir, training_dir, testing_dir):\n",
    "    file_no = 0\n",
    "    line_no = 0\n",
    "    training_or_testing_dir = training_dir\n",
    "    \n",
    "    with open(fileName) as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        \n",
    "        if line_no % 100000 == 0:\n",
    "            print(str(line_no) + \" lines ran\")\n",
    "        \n",
    "        line_no = line_no + 1            \n",
    "        \n",
    "        # Is summary line - stored under reference \n",
    "        if (line_no - 3) % 8 == 0:\n",
    "            \n",
    "            seg_list = jieba.cut(line.strip(), cut_all=False) # Accurate mode - suitable for this case\n",
    "            summary_line = \" \".join(seg_list)\n",
    "            \n",
    "        # Is short text line - stored under extractive_dataset\n",
    "        if (line_no - 6) % 8 == 0:\n",
    "            file_no = file_no + 1\n",
    "            \n",
    "            # After 70% (Training set) is created\n",
    "            if file_no == 1680415:\n",
    "                training_or_testing_dir = testing_dir\n",
    "                file_no = 1\n",
    "            \n",
    "            seg_list = jieba.cut(line.strip(), cut_all=False) # Accurate mode - suitable for this case\n",
    "            short_text = \" \".join(seg_list)\n",
    "            \n",
    "            new_filename = str(file_no) + \".txt\"\n",
    "            new_filepath = os.path.join(training_or_testing_dir, new_filename)\n",
    "            \n",
    "            with open(new_filepath, \"a\") as f:\n",
    "                    f.write(short_text.encode('utf-8') + '\\n' + '\\n' + '@highlight' + '\\n' + '\\n' + summary_line.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 lines ran\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.314 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 lines ran\n",
      "200000 lines ran\n",
      "300000 lines ran\n",
      "400000 lines ran\n",
      "500000 lines ran\n",
      "600000 lines ran\n",
      "700000 lines ran\n",
      "800000 lines ran\n",
      "900000 lines ran\n",
      "1000000 lines ran\n",
      "1100000 lines ran\n",
      "1200000 lines ran\n",
      "1300000 lines ran\n",
      "1400000 lines ran\n",
      "1500000 lines ran\n",
      "1600000 lines ran\n",
      "1700000 lines ran\n",
      "1800000 lines ran\n",
      "1900000 lines ran\n",
      "2000000 lines ran\n",
      "2100000 lines ran\n",
      "2200000 lines ran\n",
      "2300000 lines ran\n",
      "2400000 lines ran\n",
      "2500000 lines ran\n",
      "2600000 lines ran\n",
      "2700000 lines ran\n",
      "2800000 lines ran\n",
      "2900000 lines ran\n",
      "3000000 lines ran\n",
      "3100000 lines ran\n",
      "3200000 lines ran\n",
      "3300000 lines ran\n",
      "3400000 lines ran\n",
      "3500000 lines ran\n",
      "3600000 lines ran\n",
      "3700000 lines ran\n",
      "3800000 lines ran\n",
      "3900000 lines ran\n",
      "4000000 lines ran\n",
      "4100000 lines ran\n",
      "4200000 lines ran\n",
      "4300000 lines ran\n",
      "4400000 lines ran\n",
      "4500000 lines ran\n",
      "4600000 lines ran\n",
      "4700000 lines ran\n",
      "4800000 lines ran\n",
      "4900000 lines ran\n",
      "5000000 lines ran\n",
      "5100000 lines ran\n",
      "5200000 lines ran\n",
      "5300000 lines ran\n",
      "5400000 lines ran\n",
      "5500000 lines ran\n",
      "5600000 lines ran\n",
      "5700000 lines ran\n",
      "5800000 lines ran\n",
      "5900000 lines ran\n",
      "6000000 lines ran\n",
      "6100000 lines ran\n",
      "6200000 lines ran\n",
      "6300000 lines ran\n",
      "6400000 lines ran\n",
      "6500000 lines ran\n",
      "6600000 lines ran\n",
      "6700000 lines ran\n",
      "6800000 lines ran\n",
      "6900000 lines ran\n",
      "7000000 lines ran\n",
      "7100000 lines ran\n",
      "7200000 lines ran\n",
      "7300000 lines ran\n",
      "7400000 lines ran\n",
      "7500000 lines ran\n",
      "7600000 lines ran\n",
      "7700000 lines ran\n",
      "7800000 lines ran\n",
      "7900000 lines ran\n",
      "8000000 lines ran\n",
      "8100000 lines ran\n",
      "8200000 lines ran\n",
      "8300000 lines ran\n",
      "8400000 lines ran\n",
      "8500000 lines ran\n",
      "8600000 lines ran\n",
      "8700000 lines ran\n",
      "8800000 lines ran\n",
      "8900000 lines ran\n",
      "9000000 lines ran\n",
      "9100000 lines ran\n",
      "9200000 lines ran\n",
      "9300000 lines ran\n",
      "9400000 lines ran\n",
      "9500000 lines ran\n",
      "9600000 lines ran\n",
      "9700000 lines ran\n",
      "9800000 lines ran\n",
      "9900000 lines ran\n",
      "10000000 lines ran\n",
      "10100000 lines ran\n",
      "10200000 lines ran\n",
      "10300000 lines ran\n",
      "10400000 lines ran\n",
      "10500000 lines ran\n",
      "10600000 lines ran\n",
      "10700000 lines ran\n",
      "10800000 lines ran\n",
      "10900000 lines ran\n",
      "11000000 lines ran\n",
      "11100000 lines ran\n",
      "11200000 lines ran\n",
      "11300000 lines ran\n",
      "11400000 lines ran\n",
      "11500000 lines ran\n",
      "11600000 lines ran\n",
      "11700000 lines ran\n",
      "11800000 lines ran\n",
      "11900000 lines ran\n",
      "12000000 lines ran\n",
      "12100000 lines ran\n",
      "12200000 lines ran\n",
      "12300000 lines ran\n",
      "12400000 lines ran\n",
      "12500000 lines ran\n",
      "12600000 lines ran\n",
      "12700000 lines ran\n",
      "12800000 lines ran\n",
      "12900000 lines ran\n",
      "13000000 lines ran\n",
      "13100000 lines ran\n",
      "13200000 lines ran\n",
      "13300000 lines ran\n",
      "13400000 lines ran\n",
      "13500000 lines ran\n",
      "13600000 lines ran\n",
      "13700000 lines ran\n",
      "13800000 lines ran\n",
      "13900000 lines ran\n",
      "14000000 lines ran\n",
      "14100000 lines ran\n",
      "14200000 lines ran\n",
      "14300000 lines ran\n",
      "14400000 lines ran\n",
      "14500000 lines ran\n",
      "14600000 lines ran\n",
      "14700000 lines ran\n",
      "14800000 lines ran\n",
      "14900000 lines ran\n",
      "15000000 lines ran\n",
      "15100000 lines ran\n",
      "15200000 lines ran\n",
      "15300000 lines ran\n",
      "15400000 lines ran\n",
      "15500000 lines ran\n",
      "15600000 lines ran\n",
      "15700000 lines ran\n",
      "15800000 lines ran\n",
      "15900000 lines ran\n",
      "16000000 lines ran\n",
      "16100000 lines ran\n",
      "16200000 lines ran\n",
      "16300000 lines ran\n",
      "16400000 lines ran\n",
      "16500000 lines ran\n",
      "16600000 lines ran\n",
      "16700000 lines ran\n",
      "16800000 lines ran\n",
      "16900000 lines ran\n",
      "17000000 lines ran\n",
      "17100000 lines ran\n",
      "17200000 lines ran\n",
      "17300000 lines ran\n",
      "17400000 lines ran\n",
      "17500000 lines ran\n",
      "17600000 lines ran\n",
      "17700000 lines ran\n",
      "17800000 lines ran\n",
      "17900000 lines ran\n",
      "18000000 lines ran\n",
      "18100000 lines ran\n",
      "18200000 lines ran\n",
      "18300000 lines ran\n",
      "18400000 lines ran\n",
      "18500000 lines ran\n",
      "18600000 lines ran\n",
      "18700000 lines ran\n",
      "18800000 lines ran\n",
      "18900000 lines ran\n",
      "19000000 lines ran\n",
      "19100000 lines ran\n",
      "19200000 lines ran\n"
     ]
    }
   ],
   "source": [
    "create_abstractive_files(fileName, reference_dir, extractive_dir, training_dir, testing_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
