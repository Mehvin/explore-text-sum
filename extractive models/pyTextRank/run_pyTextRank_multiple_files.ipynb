{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyTextRank\n",
    "\n",
    "pyTextRank Github Link - https://github.com/ceteri/pytextrank\n",
    "\n",
    "Other sources - https://rare-technologies.com/text-summarization-in-python-extractive-vs-abstractive-techniques-revisited/\n",
    "\n",
    "This program runs pyTextRank\n",
    "\n",
    "Version 2 is only running the 30% testing dataset (Final version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging Module\n",
    "import datetime\n",
    "import logging\n",
    "    \n",
    "logger = logging.getLogger()\n",
    "    \n",
    "def setup_file_logger(log_file):\n",
    "    hdlr = logging.FileHandler(log_file)\n",
    "    formatter = logging.Formatter('%(message)s')\n",
    "    hdlr.setFormatter(formatter)\n",
    "    logger.addHandler(hdlr) \n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "def log(message):\n",
    "    #outputs to Jupyter console\n",
    "    #print('{} {}'.format(datetime.datetime.now(), message))\n",
    "    #outputs to file\n",
    "    logger.info(message)\n",
    "    \n",
    "setup_file_logger('failed_pyTextRank.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Settings\n",
    "dataset_folder_path = \"/home/s10166858/kawaijoe/Dataset/cnn-dailymail-master/extractive_dataset\"\n",
    "file_prefix = \"\"\n",
    "file_suffix = \".json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_textrank(file_path, new_filepath, extractive_path):\n",
    "    \n",
    "    ### STAGE 1 ###\n",
    "    # Perform statistical parsing/tagging on a document in JSON format\n",
    "    # Part-of-Speech Tagging and lemmatization is performed for every sentence in the document\n",
    "    import pytextrank\n",
    "    import sys\n",
    "\n",
    "    path_stage0 = extractive_path + '/' + file_path\n",
    "    path_stage1 = \"o1.json\"\n",
    "\n",
    "    # Parse one document to prep for TextRank (parse_graf)\n",
    "    # For each sentence outputs -  ParsedGraf(id, sha1, graf)\n",
    "    # graf - word in text - E.g. [17, \"bounds\", \"bound\", \"NNS\", 1, 34]\n",
    "    # stored to o1.json\n",
    "    with open(path_stage1, 'w') as f:\n",
    "        for graf in pytextrank.parse_doc(pytextrank.json_iter(path_stage0)):\n",
    "            f.write(\"%s\\n\" % pytextrank.pretty_print(graf._asdict()))\n",
    "            # to view output in this notebook\n",
    "            # print(pytextrank.pretty_print(graf))\n",
    "            \n",
    "    \n",
    "    ### STAGE 2 ###\n",
    "    # Key phrases are extracted along with their counts, and are normalized\n",
    "    # Collect and normalize the key phrases from a parsed document (RankedLexeme)\n",
    "    # E.g. [\"minimal generating sets\", 0.035356918184280925, [19, 23, 5], \"np\", 1]\n",
    "    path_stage1 = \"o1.json\"\n",
    "    path_stage2 = \"o2.json\"\n",
    "\n",
    "    graph, ranks = pytextrank.text_rank(path_stage1)\n",
    "    # pytextrank.render_ranks(graph, ranks)\n",
    "\n",
    "    with open(path_stage2, 'w') as f:\n",
    "        for rl in pytextrank.normalize_key_phrases(path_stage1, ranks):\n",
    "            f.write(\"%s\\n\" % pytextrank.pretty_print(rl._asdict()))\n",
    "            # to view output in this notebook\n",
    "            # print(pytextrank.pretty_print(rl))\n",
    "\n",
    "    # Print a graph\n",
    "    import networkx as nx\n",
    "    import pylab as plt\n",
    "\n",
    "    #nx.draw(graph, with_labels=True) \n",
    "    #plt.show()\n",
    "\n",
    "    ### STAGE 3 ###\n",
    "    # Calculate a significance weight/score for each sentence \n",
    "    # using MinHash to approximate a Jaccard distance from key phrases determined by TextRank\n",
    "    path_stage1 = \"o1.json\"\n",
    "    path_stage2 = \"o2.json\"\n",
    "    path_stage3 = \"o3.json\"\n",
    "\n",
    "    kernel = pytextrank.rank_kernel(path_stage2)\n",
    "\n",
    "    with open(path_stage3, 'w') as f:\n",
    "        for s in pytextrank.top_sentences(kernel, path_stage1):\n",
    "            f.write(pytextrank.pretty_print(s._asdict()))\n",
    "            f.write(\"\\n\")\n",
    "            # to view output in this notebook\n",
    "            # print(pytextrank.pretty_print(s._asdict()))\n",
    "\n",
    "    ### STAGE 4 ###\n",
    "    # Summarizes the document based on most significant sentences and key phrases.\n",
    "    path_stage2 = \"o2.json\"\n",
    "    path_stage3 = \"o3.json\"\n",
    "\n",
    "    phrases = \", \".join(set([p for p in pytextrank.limit_keyphrases(path_stage2, phrase_limit=12)]))\n",
    "    sent_iter = sorted(pytextrank.limit_sentences(path_stage3, sentence_limit=4), key=lambda x: x[1])\n",
    "    s = []\n",
    "\n",
    "    for sent_text, idx in sent_iter:\n",
    "        s.append(pytextrank.make_sentence(sent_text))\n",
    "    \n",
    "    graf_text = \"\\n\".join(s)\n",
    "    #return graf_text, phrases\n",
    "    #log(\"**excerpts:** %s\\n**keywords:** %s\\n\\n\" % (graf_text, phrases,))\n",
    "    #log(\"**excerpts:** %s\\n\\n\" % (graf_text))\n",
    "    \n",
    "    # To create summary files\n",
    "    with open(new_filepath, \"a\") as f:\n",
    "        f.write(graf_text.encode('utf-8'))\n",
    "        \n",
    "    os.remove(\"o1.json\")\n",
    "    os.remove(\"o2.json\")\n",
    "    os.remove(\"o3.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "system_dir = \"/home/s10166858/kawaijoe/Extractive Methods/pyTextRank for CNN & DM/All-NLP/system\"\n",
    "\n",
    "file_count = 1\n",
    "itr = 1\n",
    "\n",
    "# Removes '.json' from '1.json', '100.json', etc. and sort the numbers\n",
    "for file in sorted(os.listdir(dataset_folder_path), key=lambda name: int(name[0:-5])):\n",
    "    try:\n",
    "        new_filename = \"article\" + str(file_count) + \"_system1.txt\"\n",
    "        new_filepath = os.path.join(system_dir, new_filename)\n",
    "\n",
    "        if file.endswith(file_suffix):\n",
    "            run_textrank(file, new_filepath, dataset_folder_path)\n",
    "\n",
    "            # Count to check number of files ran\n",
    "            file_count = file_count + 1\n",
    "    except:\n",
    "        file_count = file_count + 1\n",
    "        log(file)\n",
    "        log(\"Unexpected error:\" + str(sys.exc_info()[0]))\n",
    "        log(\"\\n\")\n",
    "        continue\n",
    "\n",
    "print(\"Running completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
